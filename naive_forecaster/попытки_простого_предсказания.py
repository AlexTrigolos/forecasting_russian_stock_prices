# -*- coding: utf-8 -*-
"""Попытки простого предсказания.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkYCg74IFAwmkkuocLlIgvFkOUOimw9N

Импортирую большинство нужного, привязываюсь к s3 яндекса и считываю ранее сохраненные и предобработанные данные по акциям
"""

!pip install boto3
import copy
import os
import boto3
import traceback
import io
import json

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from botocore.exceptions import ClientError, NoCredentialsError
from traceback import format_exc

os.environ['AWS_ACCESS_KEY_ID'] = <access_key>
os.environ['AWS_SECRET_ACCESS_KEY'] = <secret_access_key>

BUCKET = 'russian-stocks-quotes'

access_key = os.getenv('AWS_ACCESS_KEY_ID')
secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
endpoint_url = 'https://storage.yandexcloud.net'

# Создание клиента S3
s3_client = boto3.client('s3',
                         region_name='ru-central1',
                         aws_access_key_id=access_key,
                         aws_secret_access_key=secret_key,
                         endpoint_url=endpoint_url)

def upload_object_to_s3(key, body):
    response = s3_client.put_object(Bucket=BUCKET, Key=key, Body=body)
    if response['ResponseMetadata']['HTTPStatusCode'] == 200:
        print(f"Успешно сохранен в {BUCKET}/{key}")
    else:
        print(f"Ошибка при сохранении: {response['ResponseMetadata']['HTTPStatusCode']}")

def upload_data_frame_to_s3(secid, data_frame, dir):
    pickle_buffer = io.BytesIO()
    data_frame.to_pickle(pickle_buffer)
    pickle_buffer.seek(0)
    data_frame_file = f'{dir}secids/{secid}/{secid}_data_frame.pkl'
    upload_object_to_s3(data_frame_file, pickle_buffer)

def upload_info_to_s3(secid, info, dir):
    info['miss_index'] = info['miss_index'].to_numpy().tolist()
    json_data = json.dumps(info)
    info_file = f'{dir}secids/{secid}/{secid}_info.pkl'
    upload_object_to_s3(info_file, json_data)

def upload_secid_names(dict_data, dir):
    json_data = json.dumps(list(dict_data.keys()))
    secid_names_file = f'{dir}secid_names.pkl'
    upload_object_to_s3(secid_names_file, json_data)

def upload_data_to_s3(dict_data, dir):
    try:
        upload_secid_names(dict_data, dir)
        for secid, data in dict_data.items():
            copy_data = copy.deepcopy(data)
            upload_data_frame_to_s3(secid, copy_data['data_frame'], dir)
            del copy_data['data_frame']
            upload_info_to_s3(secid, copy_data, dir)
    except ClientError as e:
        print(f"Произошла ошибка: {e.response['Error']['Message']}")
    except Exception as e:
        error_message = f"Неизвестная ошибка: {str(e)}"
        error_context = traceback.format_exc()
        print(f"{error_message}\nКонтекст ошибки:\n{error_context}")

def list_directories(s3_client):
    directories = set()
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=BUCKET, Delimiter='/'):
            for prefix in page.get('CommonPrefixes', []):
                directories.add(prefix.get('Prefix'))
    except NoCredentialsError:
        print("Ошибка: Неверные учетные данные.")
    except Exception as e:
        print(f"Произошла ошибка: {e}")
    return directories

def download_object_from_s3(key):
    response = s3_client.get_object(Bucket=BUCKET, Key=key)
    if response['ResponseMetadata']['HTTPStatusCode'] == 200:
        print(f"Успешно получен из {BUCKET}/{key}")
    else:
        print(f"Ошибка при получении: {response['ResponseMetadata']['HTTPStatusCode']}")
    return response['Body'].read()

def download_info_from_s3(dir, secid):
    key = f'{dir}secids/{secid}/{secid}_info.pkl'
    response = download_object_from_s3(key)
    data = json.loads(response)
    data['miss_index'] = pd.Index(np.array(data['miss_index']))
    return data

def download_data_frame_from_s3(dir, secid):
    key = f'{dir}secids/{secid}/{secid}_data_frame.pkl'
    response = download_object_from_s3(key)
    buffer = io.BytesIO(response)
    data = pd.read_pickle(buffer)
    data['TRADEDATE'] = pd.to_datetime(data['TRADEDATE'])
    return data

def download_secid_names(dir):
    key = f'{dir}secid_names.pkl'
    return json.loads(download_object_from_s3(key))

def download_data_from_s3(dir, secids=None):
    data = {}
    try:
        directories = download_secid_names(dir)
        for secid in directories:
            if secids is None or secid in secids:
                if secids is not None:
                    secids.remove(secid)
                data[secid] = download_info_from_s3(dir, secid)
                data[secid]['data_frame'] = download_data_frame_from_s3(dir, secid)
    except Exception as e:
        error_message = f"Неизвестная ошибка: {str(e)}"
        error_context = traceback.format_exc()
        print(f"{error_message}\nКонтекст ошибки:\n{error_context}")
    if secids is not None and len(secids) > 0:
        print(f'Не нашли {secids}')
    return data

data_frames = download_data_from_s3('preprocessed_data/')

"""Добавил метрики, их вывод и возврат. Хотел использовать r2, но временные ряды такая штука, которую, по-моему, лучше предсказывать построчно, чтобы предсказания использовать как данные для следующих предсказаний, а r2 не считает для массивов из одного элемента (строки/дня предсказания)"""

from sklearn.metrics import mean_squared_error, r2_score

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mape(y_true, y_pred, epsilon=1e-6):
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

def r2(y_true, y_pred):
    return r2_score(y_true, y_pred)

def metrics(y_true, y_pred):
  # корень из квадратичной ошибки, возвращает ошибку в тех же единицах, что и целевая переменная
  rmse_score = rmse(y_true, y_pred)
  # измеряет ошибку в процентах и позволяет легко интерпретировать результаты
  mape_score = mape(y_true, y_pred)
  # показывает, какая доля дисперсии зависимой переменной объясняется независимыми переменными
  # r2_score = r2(y_true, y_pred)
  # print(f"rmse = {rmse_score};\nmape = {mape_score};\nr2 = {r2_score}")
  return tuple([rmse_score, mape_score])#, r2_score])

"""Дальше идут мои попытки продумать ньюансы, если интересно можете посмотреть, но используемый мною (по крайней мере здесь) код находится под текстом `Основной код с большинством задумок`"""

from sklearn.model_selection import TimeSeriesSplit

def split_series(data):
  tscv = TimeSeriesSplit(n_splits=5)
  for train_index, test_index in tscv.split(data):
      yield data.iloc[train_index], data.iloc[test_index]

series = split_series(pd.DataFrame([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]))
# первая тренировка
print(next(series)[0])
print('!!!')
# второй тест
print(next(series)[1])
print('!!!')
# третий тренировка с тестом
print(next(series))

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

rmse_score = make_scorer(rmse, greater_is_better = False)

def fit_grid_search_with_cross_val(data, model, param_grid):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop('price', axis=1), data['price'])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

from sklearn.ensemble import RandomForestRegressor

data = pd.DataFrame({'date': pd.date_range(start='1/1/2020', periods=100),
                     'price': range(1, 101)})

data['date'] = pd.to_datetime(data['date'])
data['year'] = data['date'].dt.year
data['month'] = data['date'].dt.month
data['day'] = data['date'].dt.day
data.drop('date', axis=1, inplace=True)

model = RandomForestRegressor()
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30]
}

best_estimator = fit_grid_search_with_cross_val(data, model, param_grid)
print(data.iloc[[5, 20, 50, 80, 95]])
print(best_estimator.predict(data.iloc[[5, 20, 50, 80, 95]].drop('price', axis=1)))
metrics(data['price'], best_estimator.predict(data.drop('price', axis=1)))

from sklearn.linear_model import Ridge

model = Ridge()
param_grid = {
    'alpha': np.logspace(-4, 4, 10),  # Значения alpha от 10^-4 до 10^4
    'fit_intercept': [True, False]
}

best_estimator = fit_grid_search_with_cross_val(data, model, param_grid)
print(data.iloc[[5, 20, 50, 80, 95]])
print(best_estimator.predict(data.iloc[[5, 20, 50, 80, 95]].drop('price', axis=1)))
metrics(data['price'], best_estimator.predict(data.drop('price', axis=1)))

ydex = data_frames['YDEX']['data_frame'][['TRADEDATE', 'CLOSE']]

subset = pd.to_datetime(ydex['TRADEDATE'])
ydex = ydex.drop('TRADEDATE', axis=1)

ydex.loc[:, 'year'] = subset.dt.year
ydex.loc[:, 'month'] = subset.dt.month
ydex.loc[:, 'day'] = subset.dt.day
ydex

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

model = RandomForestRegressor()

param_grid = {
    'random_state': [42],
    'n_estimators': [50, 100, 200],
    'max_features': ['log2', 'sqrt'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

train_size = int(len(ydex) * 0.8)
train, val = ydex[:train_size], ydex[train_size:]

best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

param_grid = {
    'random_state': [42],
    'n_estimators': [50, 100, 200, 300, 400, 500],
    'max_features': ['log2', 'sqrt'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

ydex['lag_3_years'] = ydex['CLOSE'].shift(742)
ydex['lag_2_years'] = ydex['CLOSE'].shift(495)
ydex['lag_year_with_half'] = ydex['CLOSE'].shift(371)
ydex['lag_year'] = ydex['CLOSE'].shift(247)
ydex['lag_half_year'] = ydex['CLOSE'].shift(124)
ydex['lag_3_months'] = ydex['CLOSE'].shift(62)
ydex['lag_month'] = ydex['CLOSE'].shift(21)
ydex['lag_2_weeks'] = ydex['CLOSE'].shift(10)
ydex['lag_week'] = ydex['CLOSE'].shift(5)
ydex['lag_4'] = ydex['CLOSE'].shift(4)
ydex['lag_3'] = ydex['CLOSE'].shift(3)
ydex['lag_2'] = ydex['CLOSE'].shift(2)
ydex['lag_1'] = ydex['CLOSE'].shift(1)

train_size = int(len(ydex) * 0.8)
train, val = ydex[:train_size], ydex[train_size:]

best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.feature_importances_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для RandomForestRegressor')
plt.show()

ydex

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if ydex[lag].isnull().sum() > 0:
    temp = ydex.dropna(subset=[lag])
    if len(temp) < 62:
      ydex = ydex.drop(columns=[lag])
    else:
      ydex = temp
      break
ydex.reset_index()

train_size = int(len(ydex) * 0.8)
train, val = ydex[:train_size], ydex[train_size:]

model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))
values = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
hash_table = {columns[i]: values[i] for i in range(len(columns))}
sorted_columns = sorted(hash_table.keys(), key=lambda col: abs(hash_table[col]), reverse=True)
sorted_values = [float(hash_table[col]) for col in sorted_columns]

print("Отсортированные колонки:", sorted_columns)
print("Соответствующие вклады:", sorted_values)

sber = data_frames['SBER']['data_frame'][['TRADEDATE', 'CLOSE']]

subset = pd.to_datetime(sber['TRADEDATE'])
sber = sber.drop('TRADEDATE', axis=1)

sber.loc[:, 'year'] = subset.dt.year
sber.loc[:, 'month'] = subset.dt.month
sber.loc[:, 'day'] = subset.dt.day
sber

param_grid = {
    'random_state': [42],
    'n_estimators': [50, 100, 200, 300, 400, 500],
    'max_features': ['log2', 'sqrt'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

sber['lag_3_years'] = sber['CLOSE'].shift(742)
sber['lag_2_years'] = sber['CLOSE'].shift(495)
sber['lag_year_with_half'] = sber['CLOSE'].shift(371)
sber['lag_year'] = sber['CLOSE'].shift(247)
sber['lag_half_year'] = sber['CLOSE'].shift(124)
sber['lag_3_months'] = sber['CLOSE'].shift(62)
sber['lag_month'] = sber['CLOSE'].shift(21)
sber = sber.drop(['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month'], axis=1)
sber['lag_2_weeks'] = sber['CLOSE'].shift(10)
sber['lag_week'] = sber['CLOSE'].shift(5)
sber['lag_4'] = sber['CLOSE'].shift(4)
sber['lag_3'] = sber['CLOSE'].shift(3)
sber['lag_2'] = sber['CLOSE'].shift(2)
sber['lag_1'] = sber['CLOSE'].shift(1)

train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()

best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.feature_importances_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для RandomForestRegressor')
plt.show()

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if sber[lag].isnull().sum() > 0:
    temp = sber.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(sber):
      sber = sber.drop(columns=[lag])
    else:
      sber = temp
      break
sber = sber.reset_index().drop('index', axis=1)

train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

from sklearn.linear_model import Ridge

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

sber

sber['lag_3_years'] = sber['CLOSE'].shift(742)
sber['lag_2_years'] = sber['CLOSE'].shift(495)
sber['lag_year_with_half'] = sber['CLOSE'].shift(371)
sber['lag_year'] = sber['CLOSE'].shift(247)
sber['lag_half_year'] = sber['CLOSE'].shift(124)
sber['lag_3_months'] = sber['CLOSE'].shift(62)
sber['lag_month'] = sber['CLOSE'].shift(21)
sber['lag_2_weeks'] = sber['CLOSE'].shift(10)
sber['lag_week'] = sber['CLOSE'].shift(5)
sber['lag_4'] = sber['CLOSE'].shift(4)
sber['lag_3'] = sber['CLOSE'].shift(3)
sber['lag_2'] = sber['CLOSE'].shift(2)
sber['lag_1'] = sber['CLOSE'].shift(1)

from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

rmse_score = make_scorer(rmse, greater_is_better = False)

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

import xgboost as xgb

model = xgb.XGBRegressor()

param_grid = {
    'random_state': [42],
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'gamma': [0, 1, 5]
}

best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.feature_importances_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для XGBRegressor')
plt.show()

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}
sber = data_frames['SBER']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(sber['TRADEDATE'])
sber = sber.drop('TRADEDATE', axis=1)

sber.loc[:, 'year'] = subset.dt.year
sber.loc[:, 'month'] = subset.dt.month
sber.loc[:, 'day'] = subset.dt.day

sber['lag_3_years'] = sber['CLOSE'].shift(742)
sber['lag_2_years'] = sber['CLOSE'].shift(495)
sber['lag_year_with_half'] = sber['CLOSE'].shift(371)
sber['lag_year'] = sber['CLOSE'].shift(247)
sber['lag_half_year'] = sber['CLOSE'].shift(124)
sber['lag_3_months'] = sber['CLOSE'].shift(62)
sber['lag_month'] = sber['CLOSE'].shift(21)
sber['lag_2_weeks'] = sber['CLOSE'].shift(10)
sber['lag_week'] = sber['CLOSE'].shift(5)
sber['lag_4'] = sber['CLOSE'].shift(4)
sber['lag_3'] = sber['CLOSE'].shift(3)
sber['lag_2'] = sber['CLOSE'].shift(2)
sber['lag_1'] = sber['CLOSE'].shift(1)

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if sber[lag].isnull().sum() > 0:
    temp = sber.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(sber):
      sber = sber.drop(columns=[lag])
    else:
      sber = temp
      break
sber = sber.reset_index().drop('index', axis=1)

train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

scaler = MinMaxScaler()
transfor_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'CLOSE']
train[transfor_columns] = scaler.fit_transform(train[transfor_columns])
# val[transfor_columns] = scaler.transform(val[transfor_columns])

print(train)
model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

plt.figure(figsize=(10, 6))

train[transfor_columns] = scaler.inverse_transform(train[transfor_columns])
# Тренировочные данные
plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}

ydex = data_frames['YDEX']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(ydex['TRADEDATE'])
ydex = ydex.drop('TRADEDATE', axis=1)

ydex.loc[:, 'year'] = subset.dt.year
ydex.loc[:, 'month'] = subset.dt.month
ydex.loc[:, 'day'] = subset.dt.day

ydex['lag_2_weeks'] = ydex['CLOSE'].shift(10)
ydex['lag_week'] = ydex['CLOSE'].shift(5)
ydex['lag_4'] = ydex['CLOSE'].shift(4)
ydex['lag_3'] = ydex['CLOSE'].shift(3)
ydex['lag_2'] = ydex['CLOSE'].shift(2)
ydex['lag_1'] = ydex['CLOSE'].shift(1)

lags = ['lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if ydex[lag].isnull().sum() > 0:
    temp = ydex.dropna(subset=[lag])
    if len(temp) < 62:
      ydex = ydex.drop(columns=[lag])
    else:
      ydex = temp
      break
ydex = ydex.reset_index().drop('index', axis=1)

train_size = int(len(ydex) * 0.8)
train, val = ydex[:train_size], ydex[train_size:]

# scaler = MinMaxScaler()
# transfor_columns = ['lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'CLOSE']
# train[transfor_columns] = scaler.fit_transform(train[transfor_columns])

model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

plt.figure(figsize=(10, 6))

# train[transfor_columns] = scaler.inverse_transform(train[transfor_columns])
# Тренировочные данные
plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}
trnfp = data_frames['TRNFP']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(trnfp['TRADEDATE'])
trnfp = trnfp.drop('TRADEDATE', axis=1)

trnfp.loc[:, 'year'] = subset.dt.year
trnfp.loc[:, 'month'] = subset.dt.month
trnfp.loc[:, 'day'] = subset.dt.day

trnfp['lag_3_years'] = trnfp['CLOSE'].shift(742)
trnfp['lag_2_years'] = trnfp['CLOSE'].shift(495)
trnfp['lag_year_with_half'] = trnfp['CLOSE'].shift(371)
trnfp['lag_year'] = trnfp['CLOSE'].shift(247)
trnfp['lag_half_year'] = trnfp['CLOSE'].shift(124)
trnfp['lag_3_months'] = trnfp['CLOSE'].shift(62)
trnfp['lag_month'] = trnfp['CLOSE'].shift(21)
trnfp['lag_2_weeks'] = trnfp['CLOSE'].shift(10)
trnfp['lag_week'] = trnfp['CLOSE'].shift(5)
trnfp['lag_4'] = trnfp['CLOSE'].shift(4)
trnfp['lag_3'] = trnfp['CLOSE'].shift(3)
trnfp['lag_2'] = trnfp['CLOSE'].shift(2)
trnfp['lag_1'] = trnfp['CLOSE'].shift(1)

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if trnfp[lag].isnull().sum() > 0:
    temp = trnfp.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(trnfp):
      trnfp = trnfp.drop(columns=[lag])
    else:
      trnfp = temp
      break
trnfp = trnfp.reset_index().drop('index', axis=1)

train_size = int(len(trnfp) * 0.8)
train, val = trnfp[:train_size], trnfp[train_size:]

from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

scaler = MinMaxScaler()
transfor_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'CLOSE']
train[transfor_columns] = scaler.fit_transform(train[transfor_columns])
# val[transfor_columns] = scaler.transform(val[transfor_columns])

print(train)
model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

plt.figure(figsize=(10, 6))

train[transfor_columns] = scaler.inverse_transform(train[transfor_columns])
# Тренировочные данные
plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

plt.figure(figsize=(10, 6))

print(val['CLOSE'])
# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}
belu = data_frames['BELU']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(belu['TRADEDATE'])
belu = belu.drop('TRADEDATE', axis=1)

belu.loc[:, 'year'] = subset.dt.year
belu.loc[:, 'month'] = subset.dt.month
belu.loc[:, 'day'] = subset.dt.day

belu['lag_3_years'] = belu['CLOSE'].shift(742)
belu['lag_2_years'] = belu['CLOSE'].shift(495)
belu['lag_year_with_half'] = belu['CLOSE'].shift(371)
belu['lag_year'] = belu['CLOSE'].shift(247)
belu['lag_half_year'] = belu['CLOSE'].shift(124)
belu['lag_3_months'] = belu['CLOSE'].shift(62)
belu['lag_month'] = belu['CLOSE'].shift(21)
belu['lag_2_weeks'] = belu['CLOSE'].shift(10)
belu['lag_week'] = belu['CLOSE'].shift(5)
belu['lag_4'] = belu['CLOSE'].shift(4)
belu['lag_3'] = belu['CLOSE'].shift(3)
belu['lag_2'] = belu['CLOSE'].shift(2)
belu['lag_1'] = belu['CLOSE'].shift(1)

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if belu[lag].isnull().sum() > 0:
    temp = belu.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(belu):
      belu = belu.drop(columns=[lag])
    else:
      belu = temp
      break
belu = belu.reset_index().drop('index', axis=1)

train_size = int(len(belu) * 0.8)
train, val = belu[:train_size], belu[train_size:]

from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

rmse_score = make_scorer(rmse, greater_is_better = False)

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

scaler = MinMaxScaler()
transfor_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'CLOSE']
train[transfor_columns] = scaler.fit_transform(train[transfor_columns])
# val[transfor_columns] = scaler.transform(val[transfor_columns])

print(train)
model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

plt.figure(figsize=(10, 6))

train[transfor_columns] = scaler.inverse_transform(train[transfor_columns])
# Тренировочные данные
plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

plt.figure(figsize=(10, 6))

print(val['CLOSE'])
# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), best_estimator.predict(val.drop('CLOSE', axis=1)), ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

val.loc[val['lag_1'] == val['CLOSE'].shift(1), 'lag_1'] = val['lag_1'].fillna(val['lag_1'].mean())
val.loc[val['lag_2'] == val['CLOSE'].shift(2), 'lag_2'] = val['lag_2'].fillna(val['lag_2'].mean())
val.loc[val['lag_3'] == val['CLOSE'].shift(3), 'lag_3'] = val['lag_3'].fillna(val['lag_3'].mean())
val.loc[val['lag_4'] == val['CLOSE'].shift(4), 'lag_4'] = val['lag_4'].fillna(val['lag_4'].mean())
val.loc[val['lag_week'] == val['CLOSE'].shift(5), 'lag_week'] = val['lag_week'].fillna(val['lag_week'].mean())
val.loc[val['lag_2_weeks'] == val['CLOSE'].shift(10), 'lag_2_weeks'] = val['lag_2_weeks'].fillna(val['lag_2_weeks'].mean())
val.loc[val['lag_month'] == val['CLOSE'].shift(21), 'lag_month'] = val['lag_month'].fillna(val['lag_month'].mean())
val.loc[val['lag_3_months'] == val['CLOSE'].shift(62), 'lag_3_months'] = val['lag_3_months'].fillna(val['lag_3_months'].mean())
val.loc[val['lag_half_year'] == val['CLOSE'].shift(124), 'lag_half_year'] = val['lag_half_year'].fillna(val['lag_half_year'].mean())
val.loc[val['lag_year'] == val['CLOSE'].shift(247), 'lag_year'] = val['lag_year'].fillna(val['lag_year'].mean())
val.loc[val['lag_year_with_half'] == val['CLOSE'].shift(371), 'lag_year_with_half'] = val['lag_year_with_half'].fillna(val['lag_year_with_half'].mean())
val.loc[val['lag_2_years'] == val['CLOSE'].shift(495), 'lag_2_years'] = val['lag_2_years'].fillna(val['lag_2_years'].mean())
val.loc[val['lag_3_years'] == val['CLOSE'].shift(495), 'lag_3_years'] = val['lag_3_years'].fillna(val['lag_3_years'].mean())

val

param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000)
}
sber = data_frames['SBER']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(sber['TRADEDATE'])
sber = sber.drop('TRADEDATE', axis=1)

sber.loc[:, 'year'] = subset.dt.year
sber.loc[:, 'month'] = subset.dt.month
sber.loc[:, 'day'] = subset.dt.day

sber['lag_3_years'] = sber['CLOSE'].shift(742)
sber['lag_2_years'] = sber['CLOSE'].shift(495)
sber['lag_year_with_half'] = sber['CLOSE'].shift(371)
sber['lag_year'] = sber['CLOSE'].shift(247)
sber['lag_half_year'] = sber['CLOSE'].shift(124)
sber['lag_3_months'] = sber['CLOSE'].shift(62)
sber['lag_month'] = sber['CLOSE'].shift(21)
sber['lag_2_weeks'] = sber['CLOSE'].shift(10)
sber['lag_week'] = sber['CLOSE'].shift(5)
sber['lag_4'] = sber['CLOSE'].shift(4)
sber['lag_3'] = sber['CLOSE'].shift(3)
sber['lag_2'] = sber['CLOSE'].shift(2)
sber['lag_1'] = sber['CLOSE'].shift(1)

lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if sber[lag].isnull().sum() > 0:
    temp = sber.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(sber):
      sber = sber.drop(columns=[lag])
    else:
      sber = temp
      break
sber = sber.reset_index().drop('index', axis=1)

train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

val.loc[val['lag_1'] == val['CLOSE'].shift(1), 'lag_1'] = val['lag_1'].fillna(val['lag_1'].mean())
val.loc[val['lag_2'] == val['CLOSE'].shift(2), 'lag_2'] = val['lag_2'].fillna(val['lag_2'].mean())
val.loc[val['lag_3'] == val['CLOSE'].shift(3), 'lag_3'] = val['lag_3'].fillna(val['lag_3'].mean())
val.loc[val['lag_4'] == val['CLOSE'].shift(4), 'lag_4'] = val['lag_4'].fillna(val['lag_4'].mean())
val.loc[val['lag_week'] == val['CLOSE'].shift(5), 'lag_week'] = val['lag_week'].fillna(val['lag_week'].mean())
val.loc[val['lag_2_weeks'] == val['CLOSE'].shift(10), 'lag_2_weeks'] = val['lag_2_weeks'].fillna(val['lag_2_weeks'].mean())
val.loc[val['lag_month'] == val['CLOSE'].shift(21), 'lag_month'] = val['lag_month'].fillna(val['lag_month'].mean())
val.loc[val['lag_3_months'] == val['CLOSE'].shift(62), 'lag_3_months'] = val['lag_3_months'].fillna(val['lag_3_months'].mean())
val.loc[val['lag_half_year'] == val['CLOSE'].shift(124), 'lag_half_year'] = val['lag_half_year'].fillna(val['lag_half_year'].mean())
val.loc[val['lag_year'] == val['CLOSE'].shift(247), 'lag_year'] = val['lag_year'].fillna(val['lag_year'].mean())
val.loc[val['lag_year_with_half'] == val['CLOSE'].shift(371), 'lag_year_with_half'] = val['lag_year_with_half'].fillna(val['lag_year_with_half'].mean())
val.loc[val['lag_2_years'] == val['CLOSE'].shift(495), 'lag_2_years'] = val['lag_2_years'].fillna(val['lag_2_years'].mean())
val.loc[val['lag_3_years'] == val['CLOSE'].shift(495), 'lag_3_years'] = val['lag_3_years'].fillna(val['lag_3_years'].mean())

from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler

def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

scaler = MinMaxScaler()
transfor_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'CLOSE']
train[transfor_columns] = scaler.fit_transform(train[transfor_columns])
# val[transfor_columns] = scaler.transform(val[transfor_columns])

print(train)
model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для Ridge')
plt.show()

"""**Основной код с большинством задумок**

Хочу еще добавить сохранение данных в s3, сделать возможность запускать подряд для всех бумаг, сделать возможность запуска множества моделей (здесь пробовал только Ridge и RandomForestRegressor). И хочу сохранять не только метрики, но и хранить среднее значение метрики на n дней (скорее всего этот 1, 3, 5, 10/2_недели, 21/месяц, 62/квартал, скорее всего больше будет бессмысленно слабо верю, что хоть как-то возможно предсказывать даже на 3 месяца), делать среднее между разными бумагами не вижу смысла, по причинам того, что разные бумаги могут очень сильно отличаться в динамике, ну и есть те же сплиты (деление бумаг, когда вместо одной за 1000 дают 10 за 100, такое предсказать только по графикам невозможно на валидационных данных)
"""

from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

# кастомная метрика для GridSearch
rmse_score = make_scorer(rmse, greater_is_better = False)

# Параметры для обучения
# !!!!! лес !!!!!
# для сбера эти параметры 3.25 минуты, со всеми почти 2 часа должно быть
# param_grid = {
#     'random_state': [42],
#     'n_estimators': [50, 100, 200, 300, 400, 500],
#     'max_features': ['log2', 'sqrt'],
#     # 'max_depth': [None, 10, 20, 30],
#     # 'min_samples_split': [2, 5, 10],
#     # 'min_samples_leaf': [1, 2, 4],
#     'bootstrap': [True, False]
# }
# !!!!! ridge !!!!!
# пара минут
param_grid = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000),  # Значения alpha от 10^-4 до 10^4
    'fit_intercept': [True, False]
}

# Получаем данные по бумаге и удаляем дату
sber = data_frames['SBER']['data_frame'][['TRADEDATE', 'CLOSE']]
subset = pd.to_datetime(sber['TRADEDATE'])
sber = sber.drop('TRADEDATE', axis=1)

# добавляем дату по отдельности
sber.loc[:, 'year'] = subset.dt.year
sber.loc[:, 'month'] = subset.dt.month
sber.loc[:, 'day'] = subset.dt.day

# Добавляем отступы по возможным корреляциям (очень сложно выбрать нормальные отсутпы по причине того, что торги на бирже не нормированы, есть праздники, переносы, блокировки торгов, переезд компаний и другое)
# но в среднем интернет выдал 247 с хвостиком рабочих дней в году, что я уже пытался нормально разделить, например для месяца получается 21 торговый день, хоть дней примерно 30
sber['lag_3_years'] = sber['CLOSE'].shift(742)
sber['lag_2_years'] = sber['CLOSE'].shift(495)
sber['lag_year_with_half'] = sber['CLOSE'].shift(371)
sber['lag_year'] = sber['CLOSE'].shift(247)
sber['lag_half_year'] = sber['CLOSE'].shift(124)
sber['lag_3_months'] = sber['CLOSE'].shift(62)
sber['lag_month'] = sber['CLOSE'].shift(21)
sber['lag_2_weeks'] = sber['CLOSE'].shift(10)
sber['lag_week'] = sber['CLOSE'].shift(5)
sber['lag_4'] = sber['CLOSE'].shift(4)
sber['lag_3'] = sber['CLOSE'].shift(3)
sber['lag_2'] = sber['CLOSE'].shift(2)
sber['lag_1'] = sber['CLOSE'].shift(1)

# Далее убираются строки по лагам, которые не имеют данных, если данных остается меньше чем на 3 месяца или меньше 10% от начальных данных, то удаляется полностью колонка
# Потому что может быть ситуация, что данных на 3 года и 2 месяца, и только 2 месяца будут иметь лаг в 3 года, а я не хочу удалять так много данных
# Если же данных достаточно, то это будет самый большой лаг и удаляются строки, в которых по этому лагу пропуски. И переназначаем индексы
lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
for lag in lags:
  if sber[lag].isnull().sum() > 0:
    temp = sber.dropna(subset=[lag])
    if len(temp) < 62 or len(temp) * 10 < len(sber):
      sber = sber.drop(columns=[lag])
    else:
      sber = temp
      break
sber = sber.reset_index().drop('index', axis=1)

# Разбиваем данные, в валидацию идет 20%
train_size = int(len(sber) * 0.8)
train, val = sber[:train_size], sber[train_size:]

lags = { 1: 'lag_1', 2: 'lag_2', 3: 'lag_3', 4: 'lag_4', 5: 'lag_week', 10: 'lag_2_weeks',
         21: 'lag_month', 62: 'lag_3_months', 124: 'lag_half_year', 247: 'lag_year',
         371: 'lag_year_with_half', 495: 'lag_2_years', 742: 'lag_3_years' }
rev_lags = { 'lag_1': 1, 'lag_2': 2, 'lag_3': 3, 'lag_4': 4, 'lag_week': 5, 'lag_2_weeks': 10,
             'lag_month': 21, 'lag_3_months': 62, 'lag_half_year': 124, 'lag_year': 247,
             'lag_year_with_half': 371, 'lag_2_years': 495, 'lag_3_years': 742 }

# Очищаем те данные, что в валидацию попали из валидационных данных (чтобы не пытаться использовать известные реальные целевые значения для обучения)
val.loc[val['lag_1'] == val['CLOSE'].shift(1), 'lag_1'] = np.nan
val.loc[val['lag_2'] == val['CLOSE'].shift(2), 'lag_2'] = np.nan
val.loc[val['lag_3'] == val['CLOSE'].shift(3), 'lag_3'] = np.nan
val.loc[val['lag_4'] == val['CLOSE'].shift(4), 'lag_4'] = np.nan
val.loc[val['lag_week'] == val['CLOSE'].shift(5), 'lag_week'] = np.nan
val.loc[val['lag_2_weeks'] == val['CLOSE'].shift(10), 'lag_2_weeks'] = np.nan
val.loc[val['lag_month'] == val['CLOSE'].shift(21), 'lag_month'] = np.nan
val.loc[val['lag_3_months'] == val['CLOSE'].shift(62), 'lag_3_months'] = np.nan
val.loc[val['lag_half_year'] == val['CLOSE'].shift(124), 'lag_half_year'] = np.nan
val.loc[val['lag_year'] == val['CLOSE'].shift(247), 'lag_year'] = np.nan
val.loc[val['lag_year_with_half'] == val['CLOSE'].shift(371), 'lag_year_with_half'] = np.nan
val.loc[val['lag_2_years'] == val['CLOSE'].shift(495), 'lag_2_years'] = np.nan
val.loc[val['lag_3_years'] == val['CLOSE'].shift(742), 'lag_3_years'] = np.nan

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler

# Делаем GridSearch используя разделение для временных рядов и возвращаем лучшую модель
def fit_grid_search_with_cross_val(data, model, param_grid, target):
  tscv = TimeSeriesSplit(n_splits=5)
  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=2, scoring=rmse_score)
  grid_search.fit(data.drop(target, axis=1), data[target])
  print(f'Лучшие параметры: {grid_search.best_params_}')
  return grid_search.best_estimator_

# нормализуем лаги (не трогаем таргет и даты)
scaler = MinMaxScaler()
transfor_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
train[transfor_columns] = scaler.fit_transform(train[transfor_columns])
# val[transfor_columns] = scaler.transform(val[transfor_columns])

metric_scores = list()
predictions = list()

# обучаем модель
# !!!!! лес !!!!!
# model = RandomForestRegressor()
# !!!!! ridge !!!!!
model = Ridge()
best_estimator = fit_grid_search_with_cross_val(train, model, param_grid, 'CLOSE')
# делаем валидацию построчно, чтоб предсказание использовать для следующих предсказаний
for index, row in val.iterrows():
  df = pd.DataFrame([row])
  # print(df)
  # т.к. все кроме первой строки будут иметь пропуски в лагах, то перед предсказанием устанавливается лаг равный -n предсказанию
  for col in df.columns[df.isnull().any()].tolist():
    df[col] = predictions[-rev_lags[col]]
  # и делаетсянормализация
  df[transfor_columns] = scaler.transform(df[transfor_columns])
  # print(df)
  prediction = best_estimator.predict(df.drop('CLOSE', axis=1))
  # print(prediction)
  # print(df['CLOSE'])
  # записываются ошибки и предсказание
  prediction_metric = metrics(np.array([df['CLOSE']]), np.array(prediction))
  # print(prediction_metric)
  metric_scores.append({ 'rmse': prediction_metric[0], 'mape': prediction_metric[1] })
  predictions.append(prediction[0])
# print(best_estimator.predict(val.drop('CLOSE', axis=1)))
# metrics(val['CLOSE'], best_estimator.predict(val.drop('CLOSE', axis=1)))

# показывается важность каждого параметра
# !!!!! лес !!!!!
# importances = best_estimator.feature_importances_
# !!!!! ridge !!!!!
importances = best_estimator.coef_
columns = train.columns.tolist()
columns.remove('CLOSE')
feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Важность признаков')
plt.title('Важность признаков для RandomForestRegressor')
plt.show()

# predictions

# Показываются откланения от 0 для каждой из метрик
plt.figure(figsize=(15, 6))

# метрики на дни
plt.plot(list(range(1, len(metric_scores) + 1)), [float(metric['rmse']) for metric in metric_scores], ':', label='Отклонения RMSE')
plt.plot(list(range(1, len(metric_scores) + 1)), [float(metric['mape']) for metric in metric_scores], ':', label='Отклонения MAPE')

# xticks = np.arange(0, len(metric_scores), 5)  # Индексы от 0 до длины x с шагом 5
# xticks = np.arange(0, len(metric_scores), 5)
# xtick_labels = [f"{round(metric_scores[i], 2)}" for i in xticks]
# plt.xticks(xticks, xtick_labels)
plt.yticks(np.arange(0, max(np.array([list(metric.values()) for metric in metric_scores]).flatten()), 10))
plt.xticks(ticks=np.arange(1, len(metric_scores) + 1, 10), rotation=45)
# Настройка графика
plt.title('График ошибок на метриках')
plt.xlabel('День')
plt.ylabel('Значение метрики')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

# рисуются обычаемый график, график будущих цен и график предсказаний

plt.figure(figsize=(10, 6))

# Тренировочные данные
plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), predictions, ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

# Только графики реальных цен и предсказания

plt.figure(figsize=(10, 6))

# Валидационные данные
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

# Предсказания
plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), predictions, ':', color='red', label='Предсказания')

# Настройка графика
plt.title('График тренировочных и валидационных данных с предсказаниями')
plt.xlabel('День')
plt.ylabel('Стоимость')
plt.legend()
plt.grid(True)

# Показать график
plt.show()

from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import MinMaxScaler
import copy
import datetime
import pickle

# кастомная метрика для GridSearch
rmse_score = make_scorer(rmse, greater_is_better = False)

random_forest_grid_params = {
    'random_state': [42],
    'n_estimators': [50, 100, 200, 300, 400, 500],
    'max_features': ['log2', 'sqrt'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

ridge_grid_params = {
    'random_state': [42],
    'alpha': np.logspace(-4, 4, 1000),  # Значения alpha от 10^-4 до 10^4
    'fit_intercept': [True, False]
}

base_models = [
    {
        'name': 'ridge',
        'model': Ridge(),
        'grid_params': ridge_grid_params,
        'importances_name': 'coef_'
    },
     {
        'name': 'random_forest',
        'model': RandomForestRegressor(),
        'grid_params': random_forest_grid_params,
        'importances_name': 'feature_importances_'
    }
]

lags = { 1: 'lag_1', 2: 'lag_2', 3: 'lag_3', 4: 'lag_4', 5: 'lag_week', 10: 'lag_2_weeks',
        21: 'lag_month', 62: 'lag_3_months', 124: 'lag_half_year', 247: 'lag_year',
        371: 'lag_year_with_half', 495: 'lag_2_years', 742: 'lag_3_years' }
rev_lags = { 'lag_1': 1, 'lag_2': 2, 'lag_3': 3, 'lag_4': 4, 'lag_week': 5, 'lag_2_weeks': 10,
            'lag_month': 21, 'lag_3_months': 62, 'lag_half_year': 124, 'lag_year': 247,
            'lag_year_with_half': 371, 'lag_2_years': 495, 'lag_3_years': 742 }

def upload_models_data_to_s3(secid, model_name, body):
    key = f'predictions/{secid}/{model_name}.pkl'
    response = s3_client.put_object(Bucket=BUCKET, Key=key, Body=pickle.dumps(body))
    if response['ResponseMetadata']['HTTPStatusCode'] == 200:
        print(f"Успешно сохранен в {BUCKET}/{key}")
    else:
        print(f"Ошибка при сохранении: {response['ResponseMetadata']['HTTPStatusCode']}")

secids = download_secid_names('preprocessed_data/')

for secid in secids:
  # Получаем данные по бумаге и удаляем дату
  secid_data = data_frames[secid]['data_frame'][['TRADEDATE', 'CLOSE']]
  subset = pd.to_datetime(secid_data['TRADEDATE'])
  secid_data = secid_data.drop('TRADEDATE', axis=1)

  # добавляем дату по отдельности
  secid_data.loc[:, 'year'] = subset.dt.year
  secid_data.loc[:, 'month'] = subset.dt.month
  secid_data.loc[:, 'day'] = subset.dt.day

  # Добавляем отступы по возможным корреляциям (очень сложно выбрать нормальные отсутпы по причине того, что торги на бирже не нормированы, есть праздники, переносы, блокировки торгов, переезд компаний и другое)
  # но в среднем интернет выдал 247 с хвостиком рабочих дней в году, что я уже пытался нормально разделить, например для месяца получается 21 торговый день, хоть дней примерно 30
  for lag_name, lag_num in rev_lags.items():
    secid_data[lag_name] = secid_data['CLOSE'].shift(lag_num)
  # secid_data['lag_3_years'] = secid_data['CLOSE'].shift(742)
  # secid_data['lag_2_years'] = secid_data['CLOSE'].shift(495)
  # secid_data['lag_year_with_half'] = secid_data['CLOSE'].shift(371)
  # secid_data['lag_year'] = secid_data['CLOSE'].shift(247)
  # secid_data['lag_half_year'] = secid_data['CLOSE'].shift(124)
  # secid_data['lag_3_months'] = secid_data['CLOSE'].shift(62)
  # secid_data['lag_month'] = secid_data['CLOSE'].shift(21)
  # secid_data['lag_2_weeks'] = secid_data['CLOSE'].shift(10)
  # secid_data['lag_week'] = secid_data['CLOSE'].shift(5)
  # secid_data['lag_4'] = secid_data['CLOSE'].shift(4)
  # secid_data['lag_3'] = secid_data['CLOSE'].shift(3)
  # secid_data['lag_2'] = secid_data['CLOSE'].shift(2)
  # secid_data['lag_1'] = secid_data['CLOSE'].shift(1)

  # Далее убираются строки по лагам, которые не имеют данных, если данных остается меньше чем на 3 месяца или меньше 10% от начальных данных, то удаляется полностью колонка
  # Потому что может быть ситуация, что данных на 3 года и 2 месяца, и только 2 месяца будут иметь лаг в 3 года, а я не хочу удалять так много данных
  # Если же данных достаточно, то это будет самый большой лаг и удаляются строки, в которых по этому лагу пропуски. И переназначаем индексы
  # lags = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
  lag_names = list(reversed(lags.values()))
  for lag in lag_names:
    if secid_data[lag].isnull().sum() > 0:
      temp = secid_data.dropna(subset=[lag])
      if len(temp) < 62 or len(temp) * 10 < len(secid_data):
        secid_data = secid_data.drop(columns=[lag])
      else:
        secid_data = temp
        break
  secid_data = secid_data.reset_index().drop('index', axis=1)

  # Разбиваем данные, в валидацию идет 20%
  train_size = int(len(secid_data) * 0.8)
  train, val = secid_data[:train_size], secid_data[train_size:]

  valid_columns = [column for column in list(reversed(lags.values())) if column in val.columns.tolist()]
  # Очищаем те данные, что в валидацию попали из валидационных данных (чтобы не пытаться использовать известные реальные целевые значения для обучения)
  for lag_name, lag_num in rev_lags.items():
    if lag_name in valid_columns:
      val.loc[val[lag_name] == val['CLOSE'].shift(lag_num), lag_name] = np.nan
  # val.loc[val['lag_1'] == val['CLOSE'].shift(1), 'lag_1'] = np.nan
  # val.loc[val['lag_2'] == val['CLOSE'].shift(2), 'lag_2'] = np.nan
  # val.loc[val['lag_3'] == val['CLOSE'].shift(3), 'lag_3'] = np.nan
  # val.loc[val['lag_4'] == val['CLOSE'].shift(4), 'lag_4'] = np.nan
  # val.loc[val['lag_week'] == val['CLOSE'].shift(5), 'lag_week'] = np.nan
  # val.loc[val['lag_2_weeks'] == val['CLOSE'].shift(10), 'lag_2_weeks'] = np.nan
  # val.loc[val['lag_month'] == val['CLOSE'].shift(21), 'lag_month'] = np.nan
  # val.loc[val['lag_3_months'] == val['CLOSE'].shift(62), 'lag_3_months'] = np.nan
  # val.loc[val['lag_half_year'] == val['CLOSE'].shift(124), 'lag_half_year'] = np.nan
  # val.loc[val['lag_year'] == val['CLOSE'].shift(247), 'lag_year'] = np.nan
  # val.loc[val['lag_year_with_half'] == val['CLOSE'].shift(371), 'lag_year_with_half'] = np.nan
  # val.loc[val['lag_2_years'] == val['CLOSE'].shift(495), 'lag_2_years'] = np.nan
  # val.loc[val['lag_3_years'] == val['CLOSE'].shift(742), 'lag_3_years'] = np.nan


  # Делаем GridSearch используя разделение для временных рядов и возвращаем лучшую модель
  def fit_grid_search_with_cross_val(data, model, param_grid, target):
    tscv = TimeSeriesSplit(n_splits=5)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, verbose=0, scoring=rmse_score)
    grid_search.fit(data.drop(target, axis=1), data[target])
    print(f'Лучшие параметры: {grid_search.best_params_}')
    return grid_search.best_estimator_, grid_search.best_params_

  # нормализуем лаги (не трогаем таргет и даты)
  scaler = MinMaxScaler()
  # valid_columns = ['lag_3_years', 'lag_2_years', 'lag_year_with_half', 'lag_year', 'lag_half_year', 'lag_3_months', 'lag_month', 'lag_2_weeks', 'lag_week', 'lag_4', 'lag_3', 'lag_2', 'lag_1']
  train.loc[:, valid_columns] = scaler.fit_transform(train[valid_columns])
  # val[valid_columns] = scaler.transform(val[valid_columns])

  models_data = copy.deepcopy(base_models)
  for data in models_data:
    print(f"Обучение {data['model']} на {secid}")
    metric_scores = list()
    predictions = list()

    train_data = copy.deepcopy(train)
    val_data = copy.deepcopy(val)

    model = data['model']
    # обучаем модель
    best_estimator, best_params = fit_grid_search_with_cross_val(train_data, model, data['grid_params'], 'CLOSE')
    # делаем валидацию построчно, чтоб предсказание использовать для следующих предсказаний
    for index, row in val_data.iterrows():
      df = pd.DataFrame([row])
      # т.к. все кроме первой строки будут иметь пропуски в лагах, то перед предсказанием устанавливается лаг равный -n предсказанию
      for col in df.columns[df.isnull().any()].tolist():
        df[col] = predictions[-rev_lags[col]]
      # и делается нормализация
      df[valid_columns] = scaler.transform(df[valid_columns])
      prediction = best_estimator.predict(df.drop('CLOSE', axis=1))
      # записываются ошибки и предсказание
      prediction_metric = metrics(np.array([df['CLOSE']]), np.array(prediction))
      metric_scores.append({ 'rmse': prediction_metric[0], 'mape': prediction_metric[1] })
      predictions.append(prediction[0])
    data['predictions'] = predictions
    data['metric_scores'] = metric_scores
    data['importances'] = getattr(best_estimator, data['importances_name'])
    data['time'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    data['best_params'] = best_params
    data['best_model'] = best_estimator
    upload_models_data_to_s3(secid, data['name'], data)
  #   print(data)
  # print(models_data)
    # importances = data['importances']
    # columns = train_data.columns.tolist()
    # columns.remove('CLOSE')
    # data['columns'] = columns
    # feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': importances})
    # feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    # plt.figure(figsize=(10, 6))
    # plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
    # plt.xlabel('Важность признаков')
    # plt.title(f"Важность признаков модели {data['model']} в бумаге {secid}")
    # plt.show()

    # plt.figure(figsize=(15, 6))

    # # метрики на дни
    # plt.plot(list(range(1, len(metric_scores) + 1)), [float(metric['rmse']) for metric in metric_scores], ':', label='Отклонения RMSE')
    # plt.plot(list(range(1, len(metric_scores) + 1)), [float(metric['mape']) for metric in metric_scores], ':', label='Отклонения MAPE')

    # # xticks = np.arange(0, len(metric_scores), 5)  # Индексы от 0 до длины x с шагом 5
    # # xticks = np.arange(0, len(metric_scores), 5)
    # # xtick_labels = [f"{round(metric_scores[i], 2)}" for i in xticks]
    # # plt.xticks(xticks, xtick_labels)
    # max_y = max(np.array([list(metric.values()) for metric in metric_scores]).flatten())
    # plt.yticks(np.arange(0, max_y, max_y / 20))
    # plt.xticks(ticks=np.arange(1, len(metric_scores) + 1, 10), rotation=45)
    # # Настройка графика
    # plt.title('График ошибок на метриках')
    # plt.xlabel('День')
    # plt.ylabel('Значение метрики')
    # plt.legend()
    # plt.grid(True)

    # # Показать график
    # plt.show()


    # plt.figure(figsize=(10, 6))

    # # Тренировочные данные
    # plt.plot(pd.to_datetime(train[['year', 'month', 'day']]), train['CLOSE'], ':', color='blue', label='Тренировочные данные')

    # # Валидационные данные
    # plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), val['CLOSE'], ':', color='orange', label='Валидационные данные')

    # # Предсказания
    # plt.plot(pd.to_datetime(val[['year', 'month', 'day']]), predictions, ':', color='red', label='Предсказания')

    # # Настройка графика
    # plt.title('График тренировочных и валидационных данных с предсказаниями')
    # plt.xlabel('День')
    # plt.ylabel('Стоимость')
    # plt.legend()
    # plt.grid(True)

    # # Показать график
    # plt.show()

def download_models_data_from_s3(secid, model_name):
    key = f'predictions/{secid}/{model_name}.pkl'
    response = s3_client.get_object(Bucket=BUCKET, Key=key)
    if response['ResponseMetadata']['HTTPStatusCode'] == 200:
        print(f"Успешно сохранен в {BUCKET}/{key}")
        print(pickle.loads(response['Body'].read()))
    else:
        print(f"Ошибка при сохранении: {response['ResponseMetadata']['HTTPStatusCode']}")

download_models_data_from_s3('ABIO', 'random_forest')